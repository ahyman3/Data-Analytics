library(tidyr)
library(caret)
library(caret)
library(klaR)
library(rpart)
library(rpart.plot)
library(matrixStats)
library(arules)
#Reading the training file
data <- read.csv("train.csv")
#seeing if any missing data
sum(is.na(data))
#converting the label to a factor
data$label <- as.factor(data$label)
#Reading the data that will be prdicted
validation <- read.csv("test.csv")
#getting dimensions of data
dim(data)
set.seed(1000)
#Getting the index for the training data
train_ix <- createDataPartition(data$label, p = 0.8, list = F)
#Training data frame
train_df <- data[train_ix, ]
#Test data set
test_df <- data[-train_ix, ]
#getting new size of the train index
dim(train_df)
dim(test_df)
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Converting instances to 1 if greater than 0, or 0
train_nb <- ifelse(train_nb == 0, "empty", ifelse(train_nb < 30, "edge", "full"))
head(train_nb)
#Converting instances to 1 if greater than 0, or 0
train_nb <- ifelse(train_nb == 0, "empty", ifelse(train_nb < 100, "edge", "full"))
train_nb
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Converting instances to 1 if greater than 0, or 0
train_nb <- ifelse(train_nb == 0, "empty", ifelse(train_nb < 100, "edge", "full"))
head(train_nb)
#Making the columns factors
train_nb <- lapply(train_nb, factor)
#Making the columns factors
train_nb <- lapply(train_nb, as.factor)
print(col)
#Making the columns factors
for (col in colnames(train_nb)){
print(col)
}
#Making the columns factors
for (col in colnames(train_nb)){
print(col)
train_nb[,col] <- as.factor(train_nb[,col])
}
str(train_nb)
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Converting instances to 1 if greater than 0, or 0
train_nb <- ifelse(train_nb == 0, "empty", ifelse(train_nb < 100, "edge", "full"))
#Converting instances to 1 if greater than 0, or 0
train_nb <- ifelse(train_nb == 0, "empty", ifelse(train_nb < 100, "edge", "full"))
st
col
train_nb[,col] <- as.factor(train_nb[,col])
dim(train_nb)
colnames(train_nb)
colnames(train_nb$pixel779)
str(train_nb)
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Converting instances to 1 if greater than 0, or 0
train_nb <- ifelse(train_nb == 0, "empty", ifelse(train_nb < 100, "edge", "full"))
#Making the columns factors
lapply(train_nb[good_cols], factor)
dim(train_nb)
head(train_nb)
good_cols
#Making the columns factors
training_factor <- lapply(train_nb[good_cols], factor)
training_factor
#Making the columns factors
training_factor <- lapply(train_nb, as.factor)
library(plyr)
#Making the columns factors
train_test <- train_nb %>% catcolwise
str(train_test)
?factor
#Converting instances to 1 if greater than 0, or 0
makeFactor <- function(col, df){
df$col <- as.factor(df$col)
}
makeFactor("pixel15", train_nb)
df[,col] <- as.factor(df[,col])
#Converting instances to 1 if greater than 0, or 0
makeFactor <- function(col, df){
df[,col] <- as.factor(df[,col])
}
makeFactor("pixel15", train_nb)
train_nb
str(train_nb)
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Converting instances to 1 if greater than 0, or 0
train_nb <- ifelse(train_nb == 0, "empty", ifelse(train_nb < 100, "edge", "full"))
colnames(train_nb)
cols <- colnames(train_nb)
train_test <- lapply(train_nb[cols], as.factor)
train_test
train_test <- lapply(train_nb[,cols], as.factor)
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
?discretize
#Converting instances to 1 if greater than 0, or 0
train_nb %>% discretize(method = "fixed", breaks = c(-1, 0, 100, 256),
labels = c("empty", "edge", "full"))
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Converting instances to 1 if greater than 0, or 0
train_nb %>% discretize(method = "fixed", breaks = c(-1, 0, 100, 256),
labels = c("empty", "edge", "full"))
discretize(train_nb, method = "fixed", breaks = (-1, 0.1, 100, 256),
discretize(train_nb, method = "fixed", breaks = c(-1, 0.1, 100, 256),
labels = c("empty", "edge", "full"))
discretize(train_nb[, "pixel15"], method = "fixed", breaks = c(-1, 0.1, 100, 256)
discretize(train_nb[,"pixel15"], method = "fixed", breaks = c(-1, 0.1, 100, 256),
discretize(train_nb[,"pixel15"], method = "fixed", breaks = c(-1, 0.1, 100, 256),
labels = c("empty", "edge", "full"))
discretize(train_nb[,"pixel150"], method = "fixed", breaks = c(-1, 0.1, 100, 256),
labels = c("empty", "edge", "full"))
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 100, 256),
labels = c("empty", "edge", "full"))
}
str(train_nb)
head(train_nb)
#Adding the label
train_nb <- data.frame(label = label, train_nb)
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_nb, tuneGrid = grid,
trControl = tc, method = "nb")
#training the nb model
nb_model <- train(label ~ ., data = train_nb,
trControl = tc, method = "nb")
head(train_nb)
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Discretizing the columns
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 100, 256),
labels = c("empty", "edge", "full"))
}
str(train_nb)
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_nb, tuneGrid = grid,
trControl = tc, method = "nb")
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Discretizing the columns
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 256),
labels = c("empty", "mark"))
}
str(train_nb)
#Adding the label
train_nb <- data.frame(label = label, train_nb)
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_nb, tuneGrid = grid,
trControl = tc, method = "nb")
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Adding the label
train_nb <- data.frame(label = label, train_nb)
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_nb, tuneGrid = grid,
trControl = tc, method = "nb")
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 256),
labels = c("empty", "mark"))
}
#Adding the label
train_nb <- data.frame(label = label, train_nb)
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_nb, tuneGrid = grid,
trControl = tc, method = "nb")
train_nb$pixel284
?NaiveBayes
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 256),
labels = c("empty", "mark"))
}
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 256),
labels = c("empty", "mark"))
}
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = T, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_nb, tuneGrid = grid,
trControl = tc, method = "nb")
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- suppressWarnings(train(label ~ ., data = train_nb, tuneGrid = grid,
trControl = tc, method = "nb"))
library(dplyr)
library(tidyr)
library(caret)
library(klaR)
library(rpart)
library(rpart.plot)
library(matrixStats)
library(arules)
library(plyr)
#Reading the training file
data <- read.csv("train.csv")
#seeing if any missing data
sum(is.na(data))
#converting the label to a factor
data$label <- as.factor(data$label)
#Reading the data that will be prdicted
validation <- read.csv("test.csv")
#getting dimensions of data
dim(data)
set.seed(1000)
#Getting the index for the training data
train_ix <- createDataPartition(data$label, p = 0.8, list = F)
#Training data frame
train_df <- data[train_ix, ]
#Test data set
test_df <- data[-train_ix, ]
#getting new size of the train index
dim(train_df)
dim(test_df)
#Making the training parameters
tc <- trainControl(method = "cv", number = 3)
#intial decision tree model
dt <- train(label ~ ., data = train_df, trControl = tc, method = "rpart")
View(train_df)
#printing the decision tree
print(dt)
#printing the decision tree
rpart.plot(dt$finalModel)
#decreasing the complexity
#Predicting the
preds <- predict(dt, test_df, "raw")
#Confusion matrix
confusionMatrix(preds, test_df$label)
#training the nb model
nb_model <- train(label ~ ., data = train_df,
trControl = tc, method = "nb")
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Adding the label
train_nb <- data.frame(label = label, train_nb)
#Train Control
tc <- trainControl(method = "cv", number = 3)
#training the nb model
nb_model <- train(label ~ ., data = train_df,
trControl = tc, method = "nb")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(caret)
library(klaR)
library(rpart)
library(rpart.plot)
library(matrixStats)
library(arules)
library(plyr)
#Reading the training file
data <- read.csv("train.csv")
#seeing if any missing data
sum(is.na(data))
#converting the label to a factor
data$label <- as.factor(data$label)
#Reading the data that will be prdicted
validation <- read.csv("test.csv")
#getting dimensions of data
dim(data)
set.seed(1000)
#Getting the index for the training data
train_ix <- createDataPartition(data$label, p = 0.8, list = F)
#Training data frame
train_df <- data[train_ix, ]
#Test data set
test_df <- data[-train_ix, ]
#getting new size of the train index
dim(train_df)
dim(test_df)
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
#Discretizing the columns
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 256),
labels = c("empty", "mark"))
}
#Adding the label
train_nb <- data.frame(label = label, train_nb)
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- expand.grid(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_df,
trControl = tc, method = "nb")
#Making the tune grid
grid <- data.frame(fL = 1, usekernel = F, adjust = 1)
#Making the tune grid
grid <- data.frame(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(label ~ ., data = train_df, tuneGrid = grid,
trControl = tc, method = "nb")
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 256),
labels = c("empty", "mark"))
}
#training the nb model
nb_model <- train(x = train_nb, y = label, data = train_df, tuneGrid = grid,
trControl = tc, method = "nb")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(dplyr)
library(tidyr)
library(klaR)
library(rpart)
library(rpart.plot)
library(matrixStats)
library(arules)
library(arules)
library(plyr)
#Reading the training file
data <- read.csv("train.csv")
#Reading the training file
data <- read.csv("train.csv")
#Reading
dim(data)
library(dplyr)
library(tidyr)
library(caret)
library(klaR)
library(rpart)
library(rpart.plot)
library(matrixStats)
library(arules)
library(plyr)
#Reading the training file
data <- read.csv("train.csv")
#seeing if any missing data
sum(is.na(data))
#converting the label to a factor
data$label <- as.factor(data$label)
#Reading the data that will be prdicted
validation <- read.csv("test.csv")
#getting dimensions of data
dim(data)
set.seed(1000)
#Getting the index for the training data
train_ix <- createDataPartition(data$label, p = 0.8, list = F)
#Training data frame
train_df <- data[train_ix, ]
#Test data set
test_df <- data[-train_ix, ]
#getting new size of the train index
dim(train_df)
dim(test_df)
#Making the training parameters
tc <- trainControl(method = "cv", number = 3)
#intial decision tree model
dt <- train(label ~ ., data = train_df, trControl = tc, method = "rpart")
#printing the decision tree
print(dt)
#printing the decision tree
rpart.plot(dt$finalModel)
#decreasing the complexity
#List to store the bad variables
col_total <- colSums(train_df[,2:ncol(train_df)])
#Getting a list of all columns that have instances
good_cols <- names(col_total[col_total != 0])
#Getting only the labels
label <- train_df$label
#Creating a new daa frame with only those columns
train_nb <- train_df[,good_cols]
for (col in colnames(train_nb)){
train_nb[,col] <- discretize(train_nb[,col], method = "fixed", breaks = c(-1, 0.1, 256),
labels = c("empty", "mark"))
}
#Train Control
tc <- trainControl(method = "cv", number = 3)
#Making the tune grid
grid <- data.frame(fL = 1, usekernel = F, adjust = 1)
#training the nb model
nb_model <- train(x = train_nb, y = label, tuneGrid = grid,
trControl = tc, method = "nb")
